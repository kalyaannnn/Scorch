__host__ void cpu_to_cuda(Tensor* tensor)private{
    float* data_tmp;
    cudaMalloc((void **)&data_tmp, tensor -> size * sizeof(float));
    cudaMemcpy(data_temp, tensor -> data, tensor -> size * sizeof(float), cudaMemcpyHostToDevice);

    tensor -> data = data_temp;
    tensor -> device = (char*)malloc(strlen(device_str) + 1);
    strcpy(tensor -> device, device_str);

    printf("Successfully sent tensor to: %s \n", tensor -> device);
}

__host__ void cuda_to_cpu(Tensor* tensor){
    float* data_tmp = (float*)malloc(tensor -> size * sizeof(float));

    cudaMemcpy(data_tmp, tensor -> data, tensor -> size * sizeof(float), cudaMemcpyDeviceToHost);
    cudaFree(tensor -> data);

    tensor -> data = data_tmp;
    const char* device_str = "cpu";
    tensor -> device = (char*)malloc(strlen(device_str) + 1);
    strcpy(tensor -> device, device_str);

    printf("Succesfully sent tensor to %s \n", tensor -> device);
}

#define THREADS_PER_BLOCK 128

__global__ void add_tensor_cuda_kernel(float* data1, float* data2, float* result_data, int size){
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if(i < size){
        result_data[i] = data1[i] + data2[i];
    } 
}

__host__ void add_tensor_cuda(Tensor* tensor1, Tensor* tensor2, float* result_data){
    int number_of_blocks = (tensor1 -> size + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;
    add_tensor_cuda_kernel <<< number_of_blocks, THREADS_PER_BLOCK >>> (tensor1 -> data, tensor2 -> data, result_data, tensor1 -> size);

    cudaError_t error = cudaGetLastError();
    if(error != cudaSuccess){
        printf("CUDA Error : %s \n", cudaGetErrorString(error));
        exit(-1);
    }
    cudaDeviceSynchronize();
}

__global__ void sub_tensor_cuda_kernel(float* data1, float* data2, float* result_data, int size){
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if(i < size){
        result_data[i] = data1[i] - data2[i];
    }
}

__host__ void sub_tensor_cuda(Tensor* tensor1, Tensor* tensor2, float* result_data){
    int number_of_blocks = (tensor1 -> size + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;
    sub_tensor_cuda_kernel <<< number_of_blocks, THREADS_PER_BLOCK >>> (tensor1 -> data, tensor2 -> data, result_data, tensor1 -> size);

    cudaError_t error = cudaGetLastError();
    if(error != cudaSuccess){
        printf("CUDA Error : %s \n", cudaGetErrorString(error));
        exit(-1);
    }
    cudaDeviceSynchronize();
}